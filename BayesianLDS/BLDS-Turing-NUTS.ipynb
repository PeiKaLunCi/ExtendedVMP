{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing, Plots, LinearAlgebra, Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "using Random\n",
    "import Distributions: pdf, MvNormal, rand\n",
    "\n",
    "Random.seed!(1) # Set random seed\n",
    "\n",
    "T = 40 # Number of timepoints\n",
    "\n",
    "# Generative parameters\n",
    "mu_0 = [8.0, 1.0] # Prior mean\n",
    "V_0 = diagm(0=>ones(2)) # Prior covariance; diageye ensures an identity matrix of Diagonal type\n",
    "F = [1.0 0.2; \n",
    "     -0.5 0.8] # Process matrix\n",
    "H = [1.0 0.0; \n",
    "     0.0 1.0] # Observation matrix\n",
    "Q = 1e-2*diagm(0=>ones(2)) # Process noise covariance\n",
    "R = 1e-1*diagm(0=>ones(2)) # Observation noise covariance\n",
    "\n",
    "# Data\n",
    "x_hat = Vector{Vector{Float64}}(undef, T)\n",
    "y_hat = Vector{Vector{Float64}}(undef, T)\n",
    "prior_x = MvNormal(mu_0, V_0)\n",
    "process_noise_dist = MvNormal(zeros(2), Q)\n",
    "obs_noise_dist = MvNormal(zeros(2), R)\n",
    "\n",
    "x_hat[1] = rand(prior_x)\n",
    "y_hat[1] = H*x_hat[1] + rand(obs_noise_dist)\n",
    "for t = 2:T\n",
    "    x_hat[t] = F*x_hat[t-1] + rand(process_noise_dist) # Execute process\n",
    "    y_hat[t] = H*x_hat[t] + rand(obs_noise_dist) # Draw observation\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLDS (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function BLDS(y)\n",
    "    # Generative parameters\n",
    "    mu_0 = [8.0, 1.0] # Prior mean\n",
    "    V_0 = diagm(0=>ones(2)) # Prior covariance; diageye ensures an identity matrix of Diagonal type\n",
    "    F = [1.0 0.2; \n",
    "         -0.5 0.8] # Process matrix\n",
    "    H = [1.0 0.0; \n",
    "         0.0 1.0] # Observation matrix\n",
    "    Q = 1e-2*diagm(0=>ones(2)) # Process noise covariance\n",
    "    R = 1e-1*diagm(0=>ones(2)) # Observation noise covariance\n",
    "    T = 40 # Number of timepoints\n",
    "    \n",
    "    # Priors\n",
    "    a_prior ~ MvNormal(zeros(4),diagm(0=>ones(4)))\n",
    "    x = Vector{Vector}(undef,T)\n",
    "    x[1] ~ MvNormal(mu_0, V_0)\n",
    "    \n",
    "    y[1] ~ MvNormal(H*x[1], R) # Observation model\n",
    "    \n",
    "    for t = 2:T\n",
    "        x[t] ~ MvNormal(reshape(a_prior,(2,2))*x[t-1], Q) # Process model\n",
    "        y[t] ~ MvNormal(H*x[t], R) # Observation model\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.025\n",
      "└ @ Turing.Inference /Users/sakbayrak/.julia/packages/Turing/O1Pn0/src/inference/hmc.jl:195\n",
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:01:17\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chains MCMC chain (50×96×1 Array{Float64,3}):\n",
       "\n",
       "Iterations        = 1:50\n",
       "Thinning interval = 1\n",
       "Chains            = 1\n",
       "Samples per chain = 50\n",
       "parameters        = a_prior[1], a_prior[2], a_prior[3], a_prior[4], x[1][1], x[1][2], x[2][1], x[2][2], x[3][1], x[3][2], x[4][1], x[4][2], x[5][1], x[5][2], x[6][1], x[6][2], x[7][1], x[7][2], x[8][1], x[8][2], x[9][1], x[9][2], x[10][1], x[10][2], x[11][1], x[11][2], x[12][1], x[12][2], x[13][1], x[13][2], x[14][1], x[14][2], x[15][1], x[15][2], x[16][1], x[16][2], x[17][1], x[17][2], x[18][1], x[18][2], x[19][1], x[19][2], x[20][1], x[20][2], x[21][1], x[21][2], x[22][1], x[22][2], x[23][1], x[23][2], x[24][1], x[24][2], x[25][1], x[25][2], x[26][1], x[26][2], x[27][1], x[27][2], x[28][1], x[28][2], x[29][1], x[29][2], x[30][1], x[30][2], x[31][1], x[31][2], x[32][1], x[32][2], x[33][1], x[33][2], x[34][1], x[34][2], x[35][1], x[35][2], x[36][1], x[36][2], x[37][1], x[37][2], x[38][1], x[38][2], x[39][1], x[39][2], x[40][1], x[40][2]\n",
       "internals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n",
       "\n",
       "Summary Statistics\n",
       " \u001b[1m parameters \u001b[0m \u001b[1m     mean \u001b[0m \u001b[1m     std \u001b[0m \u001b[1m naive_se \u001b[0m \u001b[1m    mcse \u001b[0m \u001b[1m     ess \u001b[0m \u001b[1m    rhat \u001b[0m\n",
       " \u001b[90m     Symbol \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Missing \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\n",
       "\n",
       "  a_prior[1]     1.0141    0.0082     0.0012   missing   82.9503    0.9798\n",
       "  a_prior[2]    -0.4908    0.0109     0.0015   missing   30.3204    0.9909\n",
       "  a_prior[3]     0.2087    0.0061     0.0009   missing   60.0173    0.9798\n",
       "  a_prior[4]     0.7958    0.0060     0.0008   missing   31.0002    1.0129\n",
       "     x[1][1]     8.3533    0.1545     0.0219   missing   51.5205    0.9972\n",
       "     x[1][2]     1.0888    0.1609     0.0228   missing   31.0496    1.0001\n",
       "     x[2][1]     8.7202    0.1274     0.0180   missing   10.5382    1.0734\n",
       "     x[2][2]    -3.2706    0.1420     0.0201   missing   46.3958    0.9930\n",
       "     x[3][1]     8.1276    0.1289     0.0182   missing   37.2185    1.0743\n",
       "     x[3][2]    -6.8477    0.1638     0.0232   missing   42.0923    1.0273\n",
       "     x[4][1]     6.7752    0.1073     0.0152   missing   40.8098    1.0223\n",
       "     x[4][2]    -9.4452    0.1732     0.0245   missing   48.0339    0.9963\n",
       "     x[5][1]     4.8428    0.0935     0.0132   missing   45.7872    0.9844\n",
       "     x[5][2]   -10.8699    0.1858     0.0263   missing   44.7220    0.9893\n",
       "     x[6][1]     2.5947    0.1136     0.0161   missing   91.5438    0.9799\n",
       "     x[6][2]   -11.0042    0.1691     0.0239   missing   50.7725    0.9818\n",
       "     x[7][1]     0.3118    0.1058     0.0150   missing   43.4003    0.9982\n",
       "     x[7][2]   -10.0410    0.1652     0.0234   missing   42.2923    0.9821\n",
       "     x[8][1]    -1.7950    0.1030     0.0146   missing   43.7308    0.9936\n",
       "     x[8][2]    -8.1287    0.1452     0.0205   missing   63.7002    0.9891\n",
       "     x[9][1]    -3.4705    0.1203     0.0170   missing   23.6903    1.0569\n",
       "     x[9][2]    -5.5354    0.1339     0.0189   missing   96.6483    1.0057\n",
       "    x[10][1]    -4.6755    0.1297     0.0183   missing    8.6697    1.0834\n",
       "      ⋮           ⋮          ⋮         ⋮          ⋮         ⋮         ⋮\n",
       "\u001b[31m                                                             61 rows omitted\u001b[0m\n",
       "\n",
       "Quantiles\n",
       " \u001b[1m parameters \u001b[0m \u001b[1m     2.5% \u001b[0m \u001b[1m    25.0% \u001b[0m \u001b[1m    50.0% \u001b[0m \u001b[1m    75.0% \u001b[0m \u001b[1m    97.5% \u001b[0m\n",
       " \u001b[90m     Symbol \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m\n",
       "\n",
       "  a_prior[1]     0.9989     1.0099     1.0142     1.0190     1.0281\n",
       "  a_prior[2]    -0.5139    -0.4949    -0.4913    -0.4833    -0.4741\n",
       "  a_prior[3]     0.1976     0.2044     0.2087     0.2137     0.2186\n",
       "  a_prior[4]     0.7852     0.7921     0.7957     0.7997     0.8066\n",
       "     x[1][1]     8.0853     8.2659     8.3556     8.4616     8.6127\n",
       "     x[1][2]     0.8248     0.9756     1.0991     1.1834     1.3676\n",
       "     x[2][1]     8.4822     8.6474     8.7366     8.8094     8.9577\n",
       "     x[2][2]    -3.5795    -3.3561    -3.2888    -3.1719    -3.0521\n",
       "     x[3][1]     7.9209     8.0394     8.1119     8.2145     8.3763\n",
       "     x[3][2]    -7.1170    -6.9658    -6.8393    -6.7252    -6.5691\n",
       "     x[4][1]     6.5578     6.7032     6.7797     6.8605     6.9630\n",
       "     x[4][2]    -9.8238    -9.5369    -9.4378    -9.3604    -9.1436\n",
       "     x[5][1]     4.6711     4.7761     4.8323     4.9166     5.0020\n",
       "     x[5][2]   -11.2745   -11.0029   -10.8355   -10.7533   -10.5394\n",
       "     x[6][1]     2.4167     2.5223     2.5814     2.6655     2.8380\n",
       "     x[6][2]   -11.2984   -11.1619   -10.9749   -10.8612   -10.7562\n",
       "     x[7][1]     0.1207     0.2367     0.3157     0.3769     0.4974\n",
       "     x[7][2]   -10.3274   -10.1800   -10.0377    -9.9273    -9.7655\n",
       "     x[8][1]    -1.9686    -1.8673    -1.7885    -1.7254    -1.6244\n",
       "     x[8][2]    -8.3965    -8.2205    -8.1188    -8.0232    -7.8682\n",
       "     x[9][1]    -3.6514    -3.5353    -3.4704    -3.3882    -3.2718\n",
       "     x[9][2]    -5.7289    -5.6446    -5.5328    -5.4527    -5.3136\n",
       "    x[10][1]    -4.8625    -4.7887    -4.6579    -4.5965    -4.4625\n",
       "      ⋮           ⋮          ⋮          ⋮          ⋮          ⋮\n",
       "\u001b[31m                                                      61 rows omitted\u001b[0m\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = sample(BLDS(y_hat), NUTS(100,0.65), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chains MCMC chain (50×4×1 Array{Float64,3}):\n",
       "\n",
       "Iterations        = 1:50\n",
       "Thinning interval = 1\n",
       "Chains            = 1\n",
       "Samples per chain = 50\n",
       "parameters        = a_prior[1], a_prior[2], a_prior[3], a_prior[4]\n",
       "internals         = \n",
       "\n",
       "Summary Statistics\n",
       " \u001b[1m parameters \u001b[0m \u001b[1m    mean \u001b[0m \u001b[1m     std \u001b[0m \u001b[1m naive_se \u001b[0m \u001b[1m    mcse \u001b[0m \u001b[1m     ess \u001b[0m \u001b[1m    rhat \u001b[0m\n",
       " \u001b[90m     Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Missing \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\n",
       "\n",
       "  a_prior[1]    1.0141    0.0082     0.0012   missing   82.9503    0.9798\n",
       "  a_prior[2]   -0.4908    0.0109     0.0015   missing   30.3204    0.9909\n",
       "  a_prior[3]    0.2087    0.0061     0.0009   missing   60.0173    0.9798\n",
       "  a_prior[4]    0.7958    0.0060     0.0008   missing   31.0002    1.0129\n",
       "\n",
       "Quantiles\n",
       " \u001b[1m parameters \u001b[0m \u001b[1m    2.5% \u001b[0m \u001b[1m   25.0% \u001b[0m \u001b[1m   50.0% \u001b[0m \u001b[1m   75.0% \u001b[0m \u001b[1m   97.5% \u001b[0m\n",
       " \u001b[90m     Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\n",
       "\n",
       "  a_prior[1]    0.9989    1.0099    1.0142    1.0190    1.0281\n",
       "  a_prior[2]   -0.5139   -0.4949   -0.4913   -0.4833   -0.4741\n",
       "  a_prior[3]    0.1976    0.2044    0.2087    0.2137    0.2186\n",
       "  a_prior[4]    0.7852    0.7921    0.7957    0.7997    0.8066\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group(chain,:a_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4×1 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " 1.01406  -0.490842  0.208709  0.795834"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(group(chain,:a_prior).value.data, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 2 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F_est = reshape(mean(group(chain,:a_prior).value.data, dims=1),(2,2))\n",
    "\n",
    "using PyPlot\n",
    "fig = figure()\n",
    "g, axes = subplots(nrows=2, ncols=1)\n",
    "fs = [F_est, F]\n",
    "titles = [\"(a)\", \"(b)\"]\n",
    "\n",
    "for (i,ax) in enumerate(axes)\n",
    "    img = ax.matshow(fs[i], cmap=\"Blues\", vmin=-2.0, vmax=2.0)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(titles[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
